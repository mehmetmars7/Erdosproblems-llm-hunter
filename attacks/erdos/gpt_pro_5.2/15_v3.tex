\section{Problem 15 (v3): Erd\H{o}s alternating series $\sum_{n\ge 1} (-1)^n\, \frac{n}{p_n}$ (Round 3)}

\subsection{1) ROUND-3 OBJECTIVE}

\textbf{Path (C): obstruction/correction sharpened.}
Building on Round 2's Said--Tao reduction, the remaining obstruction is quantitative cancellation in the parity process $(-1)^{\pi(m)}$.  Round 2 produced (i) an equivalence between the Erd\H{o}s series and the parity series $\sum (-1)^{\pi(m)}/(m\log m)$, and (ii) a sufficient discrepancy bound $A(x)=O\bigl(x/(\log\log x)^{1+\varepsilon}\bigr)$ for convergence.

In Round 3, the most promising advance is to show this discrepancy-only approach is essentially \emph{sharp}:
\begin{itemize}
\item the power $1+\varepsilon$ of $\log\log$ in the sufficient condition cannot be reduced to $\le 1$ if one only controls discrepancy $A(x)$;
\item more precisely, for every $\alpha\le 1$ there are equidistributed sign sequences with $A(x)=O\bigl(x/(\log\log x)^\alpha\bigr)$ for which the weighted series $\sum \varepsilon(m)/(m\log m)$ diverges.
\end{itemize}
Thus any strategy that aims to settle Erd\H{o}s by proving a bound of the form $A(x)\ll x/(\log\log x)^\alpha$ \emph{must} reach an exponent $\alpha>1$ (or exploit additional structure beyond discrepancy).

\subsection{2) Round-2 FOUNDATION USED}

We rely on the following Round 2 results:
\begin{enumerate}
\item \textbf{Said--Tao equivalence (Round 2 Theorem 5.7):}
\[
S(x):=\sum_{n\le x} (-1)^n\frac{n}{p_n}
= \frac12\,T(x\log x)+C+o(1),\qquad
T(y):=\sum_{2\le m\le y}\frac{(-1)^{\pi(m)}}{m\log m},
\]
hence convergence of $\sum (-1)^n n/p_n$ is equivalent to convergence of $\sum (-1)^{\pi(m)}/(m\log m)$.
\item \textbf{Discrepancy criterion (Round 2 Proposition 5.8):}
if $A(x):=\sum_{2\le m\le x}(-1)^{\pi(m)}$ satisfies
\[
A(x)=O\Bigl(\frac{x}{(\log\log x)^{1+\varepsilon}}\Bigr)
\quad\text{for some }\varepsilon>0,
\]
then $T(y)$ converges, hence the Erd\H{o}s series converges.
\item \textbf{Barrier example (Round 2 Proposition 5.9):}
there exists an equidistributed sign sequence $\varepsilon(m)\in\{\pm 1\}$ for which $\sum \varepsilon(m)/(m\log m)$ diverges.
\end{enumerate}

\subsection{3) NEW INSIGHT / TOOL (ROUND 3)}

\textbf{New tool: sharpness of the $\log\log$ exponent for discrepancy-only arguments.}

Round 2 showed that a bound $A(x)=O\bigl(x/(\log\log x)^{1+\varepsilon}\bigr)$ suffices for convergence of $\sum \varepsilon(m)/(m\log m)$.  Round 3 shows the exponent $1+\varepsilon$ is essentially best possible within the class of power bounds in $\log\log$:
\begin{itemize}
\item for every $0<\alpha\le 1$, one can still have equidistribution and even the quantitative bound $A(x)=O\bigl(x/(\log\log x)^\alpha\bigr)$, while forcing divergence.
\end{itemize}
This rules out an entire class of proof attempts (those that target $A(x)=O(x/(\log\log x)^\alpha)$ with $\alpha\le 1$ and nothing deeper).

\subsection{4) ATTACK PLAN (ROUND 3)}

\textbf{Round 2 gap to close:} Proposition 5.8 gave a sufficient quantitative condition on $A(x)$, but Round 2 did not clarify whether this is in any sense minimal or sharp.

\textbf{What must be proved now:}
\begin{quote}
Construct, for each $\alpha\le 1$, an equidistributed $\{\pm1\}$-sequence $\varepsilon(m)$ with discrepancy
$A_\varepsilon(x):=\sum_{3\le m\le x}\varepsilon(m)=O\bigl(x/(\log\log x)^\alpha\bigr)$
for which
$\sum_{m\ge 3} \varepsilon(m)/(m\log m)$ diverges.
\end{quote}
Once done, we can conclude that (within discrepancy-only strategies) one needs a power strictly larger than $1$ of $\log\log$.

\subsection{5) WORK (ROUND 3)}

In this section we work with an \emph{arbitrary} sign sequence $\varepsilon(m)\in\{\pm1\}$.
Define its discrepancy
\[
A_\varepsilon(x):=\sum_{3\le m\le x}\varepsilon(m).
\]

\subsubsection{5.1 A family of sparse sets with controlled counting function and divergent $\sum 1/(m\log m)$}

\begin{lemma}[Sparse divergent set with power $\log\log$ control]
\label{lem:sparse-set}
Fix $\alpha\in(0,1]$ and define, for $k\ge 3$,
\[
 n_k := 2\Bigl\lfloor k\,(\log\log k)^\alpha\Bigr\rfloor+1,
\qquad
Y:=\{n_k: k\ge 3\} \subseteq 2\mathbb{N}+1.
\]
Then:
\begin{enumerate}
\item (Counting function) $\#(Y\cap[1,N]) = O\bigl(N/(\log\log N)^\alpha\bigr)$.
\item (Divergent weight) $\sum_{m\in Y} \frac{1}{m\log m}=\infty$.
\end{enumerate}
\end{lemma}

\begin{proof}
\emph{Step 1: monotonicity.}
For $k$ large, $k(\log\log k)^\alpha$ is strictly increasing, hence $n_k$ is strictly increasing for all $k$ beyond some absolute threshold; discarding finitely many initial terms does not affect either claim.

\emph{Step 2: counting function.}
Let $N$ be large and let $K:=\#(Y\cap[1,N])$, so $n_K\le N< n_{K+1}$.
We split into cases.

\smallskip
\noindent\underline{Case 1: $K\le N^{1/2}$.}
Then trivially $K\le N^{1/2} \le N/(\log\log N)^\alpha$ for $N$ sufficiently large (since $(\log\log N)^\alpha=o(N^{1/2})$). This gives the desired bound.

\smallskip
\noindent\underline{Case 2: $K> N^{1/2}$.}
Then $\log\log K \ge \log\log(N^{1/2}) = \log(\log N - \log 2)$.
For $N$ large this implies $\log\log K \ge \tfrac12\log\log N$.
Using $n_K \ge 2\lfloor K(\log\log K)^\alpha\rfloor \ge K(\log\log K)^\alpha$ for large $K$, we obtain
\[
N \ge n_K \ge K\,(\log\log K)^\alpha \ge K\,\Bigl(\tfrac12\log\log N\Bigr)^\alpha.
\]
Thus
\[
K \le 2^\alpha\,\frac{N}{(\log\log N)^\alpha}.
\]
Combining both cases yields $\#(Y\cap[1,N])=O\bigl(N/(\log\log N)^\alpha\bigr)$.

\emph{Step 3: divergence of $\sum_{m\in Y} 1/(m\log m)$.}
For large $k$ we have
\[
 n_k \le 2k(\log\log k)^\alpha + 1 \le 3k(\log\log k)^\alpha,
\qquad
\log n_k \le 2\log k,
\]
so
\[
\frac{1}{n_k\log n_k}
\ge \frac{c}{k\,\log k\,(\log\log k)^\alpha}
\quad\text{for some }c>0\text{ and all large }k.
\]
Therefore
\[
\sum_{m\in Y}\frac{1}{m\log m}
\ge c\sum_{k\ge k_0}\frac{1}{k\,\log k\,(\log\log k)^\alpha}.
\]
The last series diverges for $\alpha\le 1$ by the integral test: set $u:=\log\log k$, so $du = \frac{dk}{k\log k}$, and
\[
\int^{\infty}\frac{dk}{k\log k\,(\log\log k)^\alpha}
= \int^{\infty}\frac{du}{u^\alpha}
=\infty \qquad (\alpha\le 1).
\]
This proves the divergence.
\end{proof}

\subsubsection{5.2 Equidistributed sequences with small discrepancy but divergent weighted sum}

\begin{proposition}[Sharpness of the $\log\log$-exponent barrier]
\label{prop:sharpness}
Fix $\alpha\in(0,1]$. There exists an equidistributed sign sequence $\varepsilon(m)\in\{\pm1\}$ such that
\begin{enumerate}
\item $A_\varepsilon(N)=O\bigl(N/(\log\log N)^\alpha\bigr)$, and
\item the series $\sum_{m\ge 3} \varepsilon(m)/(m\log m)$ diverges.
\end{enumerate}
\end{proposition}

\begin{proof}
Let $Y\subseteq 2\mathbb{N}+1$ be the set from Lemma~\ref{lem:sparse-set}.
Define
\[
\varepsilon(m) :=
\begin{cases}
+1, & m\in Y,\\
(-1)^m, & m\notin Y.
\end{cases}
\]
\emph{Step 1: discrepancy bound and equidistribution.}
The baseline sequence $(-1)^m$ has discrepancy bounded by $1$.
Each modification at $m\in Y$ flips the sign from $(-1)^m=-1$ (since $m$ is odd) to $+1$, changing the running discrepancy by $+2$.
Thus
\[
|A_\varepsilon(N)| \le 1 + 2\#(Y\cap[1,N])
= O\Bigl(\frac{N}{(\log\log N)^\alpha}\Bigr)
\]
by Lemma~\ref{lem:sparse-set}(1). In particular $A_\varepsilon(N)=o(N)$, so $\varepsilon$ is equidistributed.

\emph{Step 2: divergence of the weighted series.}
Decompose
\[
\sum_{m\ge 3}\frac{\varepsilon(m)}{m\log m}
= \sum_{m\ge 3}\frac{(-1)^m}{m\log m}
+ \sum_{m\in Y}\frac{\varepsilon(m)-(-1)^m}{m\log m}.
\]
The first series converges by the alternating series test (since $1/(m\log m)$ decreases to $0$).
For $m\in Y\subseteq 2\mathbb{N}+1$, we have $(-1)^m=-1$ and $\varepsilon(m)=+1$, hence $\varepsilon(m)-(-1)^m=2$.
Therefore the second series equals
\[
2\sum_{m\in Y}\frac{1}{m\log m},
\]
which diverges by Lemma~\ref{lem:sparse-set}(2).
Thus the full weighted series diverges.
\end{proof}

\subsubsection{5.3 Consequence for discrepancy-only strategies}

\begin{corollary}[Critical exponent for power $\log\log$ discrepancy]
\label{cor:critical}
Consider any sign sequence $\varepsilon(m)\in\{\pm1\}$.
\begin{enumerate}
\item If $A_\varepsilon(x)=O\bigl(x/(\log\log x)^{1+\delta}\bigr)$ for some $\delta>0$, then $\sum_{m\ge 3} \varepsilon(m)/(m\log m)$ converges.
\item For every $\alpha\in(0,1]$ there exists an equidistributed $\varepsilon$ with $A_\varepsilon(x)=O\bigl(x/(\log\log x)^\alpha\bigr)$ for which the same series diverges.
\end{enumerate}
Hence, within the family of criteria of the form $A_\varepsilon(x)\ll x/(\log\log x)^\beta$, the threshold $\beta=1$ is sharp.
\end{corollary}

\begin{proof}
Part (1) is exactly Round 2 Proposition 5.8 (applied to a general sign sequence; its proof uses only partial summation and does not exploit prime structure).
Part (2) is Proposition~\ref{prop:sharpness}.
\end{proof}

\subsubsection{5.4 Back to Erd\H{o}s' series}

By Round 2 Theorem 5.7, the Erd\H{o}s series converges if and only if the parity series
\[
\sum_{m\ge 2}\frac{(-1)^{\pi(m)}}{m\log m}
\]
converges.
Corollary~\ref{cor:critical} shows that a proof based solely on a bound
\[
\sum_{m\le x}(-1)^{\pi(m)} \ll \frac{x}{(\log\log x)^\beta}
\]
would have to reach some $\beta>1$; any claim with $\beta\le 1$ is provably insufficient in general (even under equidistribution).

\subsection{6) ADVERSARIAL VERIFICATION}

\begin{itemize}
\item \textbf{Quantifiers / domains.} We start at $k\ge 3$ so $\log\log k$ is defined and positive; finitely many initial terms do not affect divergence or asymptotic discrepancy.
\item \textbf{Counting bound in Lemma~\ref{lem:sparse-set}(1).} The split into $K\le N^{1/2}$ and $K> N^{1/2}$ ensures a uniform lower bound $\log\log K \ge \tfrac12\log\log N$ in the latter case; constants can be adjusted without changing the $O(\cdot)$ conclusion.
\item \textbf{Divergence in Lemma~\ref{lem:sparse-set}(2).} The comparison $1/(n_k\log n_k)\gg 1/(k\log k\,(\log\log k)^\alpha)$ uses only crude bounds $n_k\ll k(\log\log k)^\alpha$ and $\log n_k\ll \log k$; both are valid for all large $k$.
\item \textbf{Discrepancy calculation.} Each modified odd $m\in Y$ flips the sign from $-1$ to $+1$, changing the running sum by $+2$; therefore $|A_\varepsilon(N)|\le 1+2\#(Y\cap[1,N])$ is sharp and needs no hidden assumptions.
\item \textbf{No interaction with prime structure claimed.} Proposition~\ref{prop:sharpness} concerns abstract sign sequences; it is used only to certify a barrier for discrepancy-only approaches, not to assert anything about $(-1)^{\pi(m)}$.
\end{itemize}

\subsection{7) FINAL (EXACTLY ONE)}

\textbf{UNRESOLVED (BUT STRICTLY ADVANCED).}

Round 3 strengthens the Round 2 obstruction by showing the Round 2 discrepancy criterion is essentially sharp (in the natural power scale of $\log\log$):
\begin{itemize}
\item $A(x)=O\bigl(x/(\log\log x)^{1+\varepsilon}\bigr)$ is sufficient for convergence (Round 2),
\item but for every $\alpha\le 1$ there exist equidistributed sequences with $A(x)=O\bigl(x/(\log\log x)^\alpha\bigr)$ for which $\sum \varepsilon(m)/(m\log m)$ diverges (Round 3).
\end{itemize}
Therefore any attempt to settle Erd\H{o}s via a discrepancy-only bound of the form $\sum_{m\le x}(-1)^{\pi(m)} \ll x/(\log\log x)^\beta$ must reach $\beta>1$ or exploit additional structure specific to the prime-parity sequence.

\subsection{8) COMPLETION ESTIMATE (MANDATORY)}

\noindent\textbf{COMPLETION: 75\%}

\subsection{9) REFERENCES}

\begin{thebibliography}{9}
\bibitem{TaoArxiv2023}
T. Tao,
\emph{The convergence of an alternating series of Erd\H{o}s, assuming the Hardy--Littlewood prime tuples conjecture},
arXiv:2308.07205 (2023).

\bibitem{TaoBlog2023}
T. Tao,
\emph{The convergence of an alternating series of Erd\H{o}s, assuming the Hardy--Littlewood prime tuples conjecture},
blog post (Aug 2023).

\bibitem{MO313999}
MathOverflow question 313999,
\emph{A problem of Erd\H{o}s on convergence of $\sum (-1)^n n/p_n$ and equidistribution of parity of $\pi(m)$} (Oct 2018).
\end{thebibliography}

\subsection{10) OUTPUT FORMAT}

\begin{itemize}
\item No input \texttt{x.tex} file was available in this session; this writeup continues the Round 2 investigation as provided.
\item This file is intended to be saved as \texttt{15\_v3\_solutions.tex} (or \texttt{15\_v3.tex} if your pipeline expects that name).
\item The content begins with \verb|\section| and contains no preamble, as requested.
\end{itemize}
