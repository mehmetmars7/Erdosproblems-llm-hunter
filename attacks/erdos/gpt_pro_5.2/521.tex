
\noindent\textbf{FORMAL RESTATEMENT.}
Let $(\epsilon_k)_{k\ge 0}$ be independent random variables with
\[\mathbb P(\epsilon_k=1)=\mathbb P(\epsilon_k=-1)=\tfrac12.
\]
For each $n\ge 0$ define the random degree-$n$ polynomial
\[f_n(z)=\sum_{k=0}^n \epsilon_k z^k.
\]
Let $R_n$ be the number of real roots of $f_n$ (counted with multiplicity).
Question: Is it true that, with probability $1$,
\[\lim_{n\to\infty} \frac{R_n}{\log n} = \frac{2}{\pi}\ ?\]
(Here $\log$ is the natural logarithm; the limit is interpreted for integers $n\to\infty$.)

\bigskip
\noindent\textbf{QUICK LITERATURE/CONTEXT CHECK.}
The problem statement reports that Erd\H{o}s and Offord showed the number of real roots of a random degree $n$ polynomial with $\pm 1$ coefficients is $(\tfrac{2}{\pi}+o(1))\log n$. It also reports that Do proved an almost sure law for the number of real roots in $[-1,1]$, namely $R_n[-1,1]/\log n\to 1/\pi$ almost surely (for the $\pm 1$ model).

\bigskip
\noindent\textbf{ATTACK PLAN.}
\begin{itemize}
\item \emph{Proof track idea 1:} Show $R_n$ concentrates sharply around its mean and then upgrade to an almost sure limit via Borel--Cantelli (requires tail bounds and some quasi-independence across $n$).
\item \emph{Proof track idea 2:} Express $R_n$ via Kac--Rice type integrals for real zeros, then prove a strong law by analyzing fluctuations.
\item \emph{Disproof track:} Look for a mechanism creating infinitely many exceptional $n$ with unusually many real zeros (e.g. frequent near-factorizations or many sign changes), preventing convergence of $R_n/\log n$.
\end{itemize}

\bigskip
\noindent\textbf{WORK.}

\medskip
\noindent\textbf{Fast reality check (brute force small $n$; numerical root-finding).}
For small $n$ we enumerated all $2^{n+1}$ sign choices and counted real roots numerically (counting a root as real if $|\Im r|<10^{-8}$). We also counted roots in the closed unit disk (for connection with Problem~522).
\begin{verbatim}
{'n': 4, 'num_polynomials': 32, 'avg_real': 1.25, 'avg_in': 2.375, 'min_real': 0, 'max_real': 2, 'min_in': 1, 'max_in': 4}
{'n': 6, 'num_polynomials': 128, 'avg_real': 1.5, 'avg_in': 3.25, 'min_real': 0, 'max_real': 2, 'min_in': 1, 'max_in': 6}
{'n': 8, 'num_polynomials': 512, 'avg_real': 1.609375, 'avg_in': 4.3984375, 'min_real': 0, 'max_real': 4, 'min_in': 1, 'max_in': 8}
{'n': 10, 'num_polynomials': 2048, 'avg_real': 1.7265625, 'avg_in': 5.09375, 'min_real': 0, 'max_real': 4, 'min_in': 1, 'max_in': 10}
\end{verbatim}
A larger-$n$ Monte Carlo with 200 samples (same numerical criterion) gave:
\begin{verbatim}
{'n': 20, 'samples': 200, 'avg_real': 2.14, 'min_real': 0, 'max_real': 4, 'avg_in': 10.17, 'min_in': 5, 'max_in': 15}
{'n': 50, 'samples': 200, 'avg_real': 2.71, 'min_real': 0, 'max_real': 6, 'avg_in': 25.145, 'min_in': 18, 'max_in': 32}
{'n': 100, 'samples': 200, 'avg_real': 3.23, 'min_real': 0, 'max_real': 8, 'avg_in': 49.77, 'min_in': 38, 'max_in': 64}
\end{verbatim}
These are sanity checks only; numerical root-finding can misclassify roots extremely near the real axis.

\medskip
\noindent\textbf{Lemma 521.1 (root modulus annulus for Littlewood polynomials).}
Fix $n\ge 0$ and any choice of coefficients $\epsilon_0,\dots,\epsilon_n\in\{-1,1\}$. Let
\[P(z)=\sum_{k=0}^n \epsilon_k z^k.
\]
Then every complex root $z$ of $P$ satisfies
\[\tfrac12\le |z|\le 2.
\]

\noindent\textbf{Proof.}
Write $P(z)=\epsilon_n z^n + \epsilon_{n-1}z^{n-1}+\cdots+\epsilon_0$ with $\epsilon_n\ne 0$.
Cauchy's root bound states that every root $z$ satisfies
\[|z|\le 1+\max_{0\le k\le n-1} \Big|\frac{\epsilon_k}{\epsilon_n}\Big|.
\]
Here $|\epsilon_k/\epsilon_n|=1$, so $|z|\le 2$.

For the lower bound, consider the reciprocal polynomial
\[Q(w)=w^n P(1/w)=\epsilon_0 w^n + \epsilon_1 w^{n-1}+\cdots+\epsilon_n.
\]
If $z\ne 0$ is a root of $P$, then $w=1/z$ is a root of $Q$. Applying the same Cauchy bound to $Q$ (now the leading coefficient is $\epsilon_0\ne 0$ and all other coefficients also have modulus $1$) yields $|w|\le 2$. Thus $|z|=|1/w|\ge 1/2$.
\hfill$\square$

\medskip
\noindent\textbf{Lemma 521.2 (exact probability of a root at $\pm 1$).}
For each $n\ge 0$,
\begin{align*}
\mathbb P\big(f_n(1)=0\big)
&=\begin{cases}
0,& n\ \text{even},\\
2^{-(n+1)}\binom{n+1}{(n+1)/2},& n\ \text{odd},
\end{cases}\\
\mathbb P\big(f_n(-1)=0\big)
&=\begin{cases}
0,& n\ \text{even},\\
2^{-(n+1)}\binom{n+1}{(n+1)/2},& n\ \text{odd}.
\end{cases}
\end{align*}

\noindent\textbf{Proof.}
We have $f_n(1)=\sum_{k=0}^n \epsilon_k$. This sum equals $0$ exactly when there are equally many $+1$ and $-1$ among the $n+1$ coefficients. If $n$ is even then $n+1$ is odd, so equality is impossible and the probability is $0$. If $n$ is odd then $n+1$ is even and the number of sign choices with exactly $(n+1)/2$ plus signs is $\binom{n+1}{(n+1)/2}$. Dividing by the total $2^{n+1}$ sign choices gives the stated probability.

For $-1$, observe $f_n(-1)=\sum_{k=0}^n \epsilon_k(-1)^k$. Define $\epsilon_k' = \epsilon_k(-1)^k$. Then $(\epsilon_k')_{0\le k\le n}$ are still independent uniform random signs, so $f_n(-1)$ has the same distribution as $\sum_{k=0}^n \epsilon_k'$, and the same counting argument applies.
\hfill$\square$

\medskip
\noindent\textbf{Lemma 521.3 (reciprocal symmetry of real-root location in expectation).}
Let $N_n(a,b)$ denote the number of real roots of $f_n$ in an interval $(a,b)$, counted with multiplicity. Then
\[\mathbb E\,N_n(0,1)=\mathbb E\,N_n(1,\infty)
\quad\text{and}\quad
\mathbb E\,N_n(-1,0)=\mathbb E\,N_n(-\infty,-1).
\]

\noindent\textbf{Proof.}
Define the reversed polynomial
\[f_n^*(z)=z^n f_n(1/z)=\sum_{k=0}^n \epsilon_k z^{n-k}.
\]
The coefficient vector of $f_n^*$ is $(\epsilon_n,\dots,\epsilon_0)$, which has the same distribution as $(\epsilon_0,\dots,\epsilon_n)$ because the $\epsilon_k$ are i.i.d.
Hence $f_n^*$ has the same distribution as $f_n$.

If $x\ne 0$ is a real root of $f_n$, then $1/x$ is a real root of $f_n^*$, with the same multiplicity. Therefore the number of roots of $f_n$ in $(0,1)$ equals the number of roots of $f_n^*$ in $(1,\infty)$. Taking expectations and using equality in distribution gives
\[\mathbb E\,N_n(0,1)=\mathbb E\,N_n^*(1,\infty)=\mathbb E\,N_n(1,\infty).
\]
The negative interval identity is identical.
\hfill$\square$

\bigskip
\noindent\textbf{VERIFICATION.}
\begin{itemize}
\item Lemma~521.1: Cauchy's bound applies to any polynomial with nonzero leading coefficient; here leading and constant coefficients are $\pm 1$, so reciprocal polynomial is well-defined and has nonzero leading coefficient.
\item Lemma~521.2: We explicitly handled the parity obstruction. The argument for $-1$ uses the bijection $\epsilon_k\mapsto\epsilon_k(-1)^k$; independence and uniformity are preserved.
\item Lemma~521.3: The only edge case is the root $0$, but $f_n(0)=\epsilon_0\ne 0$, so there is no root at $0$.
\end{itemize}

\bigskip
\noindent\textbf{FINAL.} \textbf{UNRESOLVED}

(i) \emph{Strongest proved partial result:}
Deterministic root localization $\tfrac12\le |z|\le 2$ for every realization (Lemma~521.1), and exact formulas for the probabilities of having a real root at $\pm 1$ (Lemma~521.2). Also, a reciprocal symmetry identity in expectation for the distribution of real roots across $(0,1)$ vs $(1,\infty)$ (Lemma~521.3).

(ii) \emph{First gap (crisp):}
Prove almost sure convergence of $R_n/\log n$ to a constant (and identify it as $2/\pi$), i.e. show that fluctuations of $R_n$ around its typical size are $o(\log n)$ almost surely.

(iii) \emph{Top 3 next moves (concrete):}
\begin{enumerate}
\item Establish a tail bound of the form $\mathbb P(|R_n-\mathbb E R_n|\ge t)\le \exp(-c t^2/\log n)$ (or similar) for $t$ up to $\asymp \log n$, sufficient for Borel--Cantelli.
\item Show approximate independence (or mixing) for the increments $R_{n+\Delta}-R_n$ on a sparse subsequence (e.g. dyadic $n$) to control almost sure behavior.
\item Computation: push exact enumeration of $\mathbb E R_n$ for all polynomials up to the largest feasible $n$ (e.g. $n\approx 16$) using Sturm sequences (exact real-root counting) to reduce numerical uncertainty.
\end{enumerate}

(iv) \emph{Minimal counterexample structure:}
A counterexample would likely require infinitely many $n$ where $R_n$ deviates from $(2/\pi)\log n$ by a positive proportion of $\log n$; concretely, one would need a mechanism producing (with positive probability) unusually many near-repeated sign patterns causing extra real zeros, and such events must recur infinitely often.


\section*{1) FORMAL RESTATEMENT}

Let
\[
\Omega:=\{-1,+1\}^{\mathbb N_0},\qquad \mathbb N_0:=\{0,1,2,\dots\},
\]
equipped with the product $\sigma$--algebra and the product probability measure $\mathbb P$ under which the coordinate maps
\[
\epsilon_k(\omega):=\omega_k\in\{-1,+1\}\quad (k\in\mathbb N_0)
\]
are independent and satisfy $\mathbb P(\epsilon_k=1)=\mathbb P(\epsilon_k=-1)=\tfrac12$.

For each integer $n\ge 0$, define the random polynomial
\[
f_n(\omega;z):=\sum_{k=0}^n \epsilon_k(\omega)\,z^k\in\mathbb Z[z].
\]

Define $R_n(\omega)$ to be the number of \emph{real} zeros of $f_n(\omega;\cdot)$ \emph{counted with multiplicity}, i.e.
\[
R_n(\omega):=\sum_{x\in\mathbb R:\ f_n(\omega;x)=0}\operatorname{mult}_x(f_n(\omega;\cdot)),
\]
where $\operatorname{mult}_x$ denotes the multiplicity of $x$ as a root of the polynomial.

Since $\log 1=0$, the ratio $R_n/\log n$ is only meaningful for $n\ge 2$. The question is:

\medskip
\noindent\textbf{Problem.}
Is it true that
\[
\mathbb P\left(\lim_{n\to\infty}\frac{R_n}{\log n}=\frac{2}{\pi}\right)=1 \ ?
\]
(Here $\log$ denotes the natural logarithm and the limit is taken over integers $n\to\infty$.)
\medskip

\noindent\textbf{Ambiguity / minimal correction.}
Many sources define $N_n(I)$ as the number of real zeros in $I$ \emph{without} multiplicity (i.e.\ counting distinct real roots). For the Bernoulli $\pm1$ Kac model, this distinction is asymptotically irrelevant: the probability of a real multiple root is $O(n^{-2})$, hence summable, so almost surely only finitely many $n$ have a real multiple root. Consequently, the existence/value of $\lim_{n\to\infty}R_n/\log n$ is equivalent to the same statement with ``counted with multiplicity'' replaced by ``counted without multiplicity'' (proved below).

\section*{2) QUICK LITERATURE/CONTEXT CHECK}

\begin{itemize}
\item The problem in the $\{\pm1\}$ formulation is listed as \textbf{OPEN} on \texttt{erdosproblems.com} as of 19 Oct 2025.
\item For Kac polynomials with i.i.d.\ mean-zero, variance-one coefficients (including $\pm1$), Do (2024) proves a strong law on $[-1,1]$:
\[
\frac{N_n([-1,1])}{\log n}\to\frac1\pi\quad\text{a.s.}
\]
\item Do also records lacunary (exponentially spaced) almost sure convergence statements for certain intervals, and indicates analogous lacunary convergence phenomena for other intervals including $\mathbb R$.
\item Can--Nguyen (arXiv:2311.15446) prove concentration inequalities for the number of real zeros, including for $N_n(\mathbb R)$: a polynomial lower-tail bound and an upper-tail bound of order $\exp(-c\sqrt{\log n})$ for deviations of order $\varepsilon\log n$.
\item Do--Nguyen--Vu (arXiv:1409.4128) prove that for broad atom distributions including Bernoulli $\pm1$,
\[
\mathbb E N_n(\mathbb R)=\frac{2}{\pi}\log n + C + o(1),
\]
and also show the probability of real double roots in the Bernoulli model is $p_{\pm1}+n^{-\omega(1)}$ with $p_{\pm1}=\Theta(n^{-2})$ on the relevant congruence class, hence $O(n^{-2})$ overall.
\item Do explains why his method does \emph{not} extend from $[-1,1]$ to all of $\mathbb R$: although $p_n$ and the reciprocal $p_n^*(x)=x^n p_n(1/x)$ have the same distribution for each fixed $n$, the \emph{processes} $(p_n)$ and $(p_n^*)$ do not have the same joint law; the ``pairing argument'' used for the maximal inequality does not extend, and new ideas are likely needed.
\end{itemize}

\section*{3) ATTACK PLAN}

\noindent\textbf{Proof-track candidates.}
\begin{enumerate}
\item Prove an almost sure limit along a lacunary subsequence $n_k$ (e.g.\ $n_k=2^k$) using Can--Nguyen tail bounds plus Borel--Cantelli, and identify the limit $2/\pi$ using the expectation asymptotic of Do--Nguyen--Vu.
\item To resolve the \emph{full} problem, mimic Do's upgrade from lacunary to full sequence via a maximal inequality on a dyadic block $[n,cn]$, but for outside real roots $[1,\infty)\cup(-\infty,-1]$. Do indicates the required pairing step breaks because it would require control for the reciprocal-polynomial \emph{process}.
\item Attempt an ergodic/martingale argument for the outside roots; no gap-free path is currently clear.
\end{enumerate}

\noindent\textbf{Disproof-track candidates.}
\begin{enumerate}
\item Try to show outside-root counts fluctuate by $\gg\log n$ infinitely often due to fresh randomness in tail coefficients; this would require rigorous lower bounds on large deviations and dependence-controlled Borel--Cantelli.
\item Try a switching/resampling construction to force oscillations; again, one needs quantitative locality/stability of root counts.
\end{enumerate}

\noindent\textbf{Choice.}
Execute proof-track (1): prove lacunary a.s.\ convergence on $\mathbb R$ and show multiplicity is irrelevant a.s.; isolate the precise missing maximal-inequality step needed for the full statement.

\section*{4) WORK}

\subsection*{PHASE 0: Hygiene and stress points}
The growth $\asymp \log n$ arises from neighborhoods of $\pm1$.
A key stress point is ``counted with multiplicity'' versus distinct roots, because discrete coefficients allow double roots with positive probability; this must be handled explicitly.

\subsection*{PHASE 1: Fast reality check (tiny $n$)}
Exact enumeration for $n\le 12$ (via exact real-root counting) yields average $R_n$ consistent with $\asymp \log n$ growth at small $n$; this is only a sanity check and does not address the a.s.\ limit.

\subsection*{PHASE 2: Landscape (tools)}
Relevant tools include Kac--Rice/Edelman--Kostlan for expectation, universality, concentration inequalities, root repulsion/no-near-double-roots, Borel--Cantelli, and maximal inequalities over dyadic blocks.

\subsection*{Lemma 1 (Multiplicity is asymptotically irrelevant almost surely)}
Let $D_n$ be the event that $f_n$ has a real multiple root. Then
\[
\sum_{n=1}^\infty \mathbb P(D_n) < \infty.
\]
Consequently, by the first Borel--Cantelli lemma,
\[
\mathbb P(D_n \text{ infinitely often}) = 0.
\]
In particular, with probability $1$ there exists $n_0(\omega)$ such that for all $n\ge n_0(\omega)$, all real roots of $f_n(\omega;\cdot)$ are simple, hence counting with multiplicity equals counting without multiplicity for all large $n$.

\paragraph{Proof.}
By Do--Nguyen--Vu for Bernoulli coefficients:
\[
\mathbb P(\text{$f_n$ has a real double root}) = p_{\pm1} + n^{-\omega(1)},
\]
where $p_{\pm1}=\Theta(n^{-2})$ when $4\mid(n+1)$ and $p_{\pm1}=0$ otherwise. Hence there exists $C_1>0$ and $n_1$ such that for all $n\ge n_1$,
\[
\mathbb P(D_n)\le C_1 n^{-2} + n^{-3}\le (C_1+1)n^{-2}.
\]
Thus $\sum_{n\ge 1}\mathbb P(D_n)<\infty$, and Borel--Cantelli implies $\mathbb P(D_n\ \text{i.o.})=0$. Therefore almost surely for all large $n$ the real roots are simple, so multiplicity-counting agrees with distinct-root counting eventually.
\hfill$\square$

\subsection*{Lemma 2 (Expectation asymptotic and identification of the constant)}
Let $N_n(\mathbb R)$ be the number of real roots of $f_n$ counted without multiplicity (equivalently with multiplicity for all large $n$ a.s.\ by Lemma~1). Then
\[
\mathbb E\,N_n(\mathbb R)=\frac{2}{\pi}\log n + C + o(1)
\quad\text{as }n\to\infty,
\]
for some constant $C\in\mathbb R$. In particular,
\[
\lim_{n\to\infty}\frac{\mathbb E\,N_n(\mathbb R)}{\log n}=\frac{2}{\pi}.
\]

\paragraph{Justification.}
This is stated in Do--Nguyen--Vu for a broad class of atom distributions including Bernoulli $\pm1$. Dividing by $\log n$ yields the displayed limit.
\hfill$\square$

\subsection*{Lemma 3 (Two-sided tail bound at scale $\varepsilon\log n$)}
For the Bernoulli $\pm1$ Kac polynomials, for every fixed $\varepsilon>0$ there exist constants $c,C>0$ such that for all $n$,
\[
\mathbb P\Big(|N_n(\mathbb R)-\mathbb E N_n(\mathbb R)|\ge \varepsilon\log n\Big)
\le C n^{-c} + C\exp\big(-c\sqrt{\log n}\big).
\]

\paragraph{Proof.}
Can--Nguyen prove
\[
\mathbb P\big(N_n(\mathbb R)\le \mathbb E N_n(\mathbb R)-\varepsilon\log n\big)\le C n^{-c}
\]
and
\[
\mathbb P\big(N_n(\mathbb R)\ge \mathbb E N_n(\mathbb R)+\varepsilon\log n\big)\le C\exp(-c\sqrt{\log n}).
\]
Adding yields the claimed two-sided bound.
\hfill$\square$

\subsection*{Theorem 4 (Lacunary almost sure law on $\mathbb R$)}
Fix a lacunary integer sequence $(n_k)_{k\ge 1}$ satisfying $n_k\to\infty$ and
\[
\exists q>1\ \forall k\ \ (n_{k+1}\ge q\,n_k).
\]
Then for the Bernoulli $\pm1$ Kac polynomials,
\[
\frac{R_{n_k}}{\log n_k}\longrightarrow \frac{2}{\pi}\quad\text{almost surely.}
\]

\paragraph{Proof.}
Let $N_n(\mathbb R)$ count real roots without multiplicity. By Lemma~1, almost surely there exists $k_0(\omega)$ such that for all $k\ge k_0(\omega)$,
\[
R_{n_k}(\omega)=N_{n_k}(\mathbb R)(\omega).
\]
Fix $\varepsilon>0$ and define
\[
A_k:=\Big\{|N_{n_k}(\mathbb R)-\mathbb E N_{n_k}(\mathbb R)|\ge \varepsilon\log n_k\Big\}.
\]
By Lemma~3,
\[
\mathbb P(A_k)\le C n_k^{-c}+C\exp\big(-c\sqrt{\log n_k}\big).
\]
Since $n_k\ge n_1 q^{k-1}$, the series $\sum_k n_k^{-c}$ converges. Also $\log n_k\ge \log n_1 + (k-1)\log q$, so $\exp(-c\sqrt{\log n_k})\le \exp(-c'\sqrt{k})$ for some $c'>0$ and all large $k$, hence $\sum_k \exp(-c\sqrt{\log n_k})<\infty$. Thus $\sum_k\mathbb P(A_k)<\infty$.

By the first Borel--Cantelli lemma, $\mathbb P(A_k\ \text{i.o.})=0$, so almost surely there exists $k_1(\omega)$ such that for all $k\ge k_1(\omega)$,
\[
|N_{n_k}(\mathbb R)-\mathbb E N_{n_k}(\mathbb R)|<\varepsilon\log n_k.
\]
Divide by $\log n_k$:
\[
\left|\frac{N_{n_k}(\mathbb R)}{\log n_k}-\frac{\mathbb E N_{n_k}(\mathbb R)}{\log n_k}\right|<\varepsilon.
\]
By Lemma~2, $\mathbb E N_{n_k}(\mathbb R)/\log n_k\to 2/\pi$. Since $\varepsilon>0$ was arbitrary,
\[
\frac{N_{n_k}(\mathbb R)}{\log n_k}\to\frac{2}{\pi}\quad\text{a.s.}
\]
Finally, $R_{n_k}=N_{n_k}(\mathbb R)$ for all large $k$ almost surely by Lemma~1, hence the same limit holds for $R_{n_k}$.
\hfill$\square$

\subsection*{What this does not prove (the central gap)}
To extend lacunary convergence to full-sequence convergence, one typically needs a maximal inequality on dyadic blocks, e.g.\ a summable bound of the form
\[
\sum_{j=1}^\infty
\mathbb P\Big(\max_{2^j\le n\le 2^{j+1}}
\big|R_n - R_{2^j}\big|\ge \varepsilon j\Big)<\infty.
\]
Do proves such a maximal control for root counts in $[-1,1]$ but notes the method does not extend to outside roots because it would require control of the reciprocal-polynomial \emph{process}, not just the one-time marginal law.

\section*{5) VERIFICATION}

\begin{itemize}
\item Lemma~1 uses a summable bound on $\mathbb P(D_n)$ and Borel--Cantelli; no independence is required for this direction.
\item Theorem~4 uses lacunarity essentially to make the upper-tail bound $\exp(-c\sqrt{\log n_k})$ summable; this is not available along the full sequence.
\item The constant $2/\pi$ arises only from the expectation asymptotic in Lemma~2; dividing by $\log n$ is legitimate for $n\ge 2$.
\item Quantifiers: Theorem~4 is an almost sure statement about the random infinite sign sequence $(\epsilon_k)_{k\ge 0}$.
\end{itemize}

\section*{6) FINAL}

\noindent\textbf{UNRESOLVED}

\medskip
\noindent (i) \emph{Strongest fully proved partial result.}
For any lacunary subsequence $(n_k)$ with $n_{k+1}\ge q n_k$ for some $q>1$,
\[
\frac{R_{n_k}}{\log n_k}\to\frac{2}{\pi}\quad\text{a.s.}
\]
Also, almost surely only finitely many $n$ have real multiple roots, so for all large $n$ the multiplicity-count equals the distinct-root count.

\medskip
\noindent (ii) \emph{First gap (crisp).}
Prove a maximal inequality/gap-filling estimate for the outside-root counts (e.g.\ on $[1,\infty)$ and $(-\infty,-1]$) strong enough to upgrade lacunary a.s.\ convergence to full-sequence a.s.\ convergence.

\medskip
\noindent (iii) \emph{Top 3 next moves.}
\begin{enumerate}
\item Develop a maximal inequality for outside intervals via a new coupling/switching argument.
\item Prove a quantitative locality/stability lemma for root counts in exterior dyadic shells under coefficient modifications outside a controlled window.
\item Attempt a disproof by proving non-summable lower bounds on deviations of size $\varepsilon\log n$ together with weak dependence to trigger infinitely-often deviations.
\end{enumerate}

\medskip
\noindent (iv) \emph{Likely minimal counterexample structure.}
Any counterexample would likely manifest through instability of outside roots $(|x|>1)$ caused by fresh randomness in high-degree coefficients, producing limsup/liminf separation of $R_n/\log n$.

\section*{7) COMPLETION ESTIMATE}

\noindent COMPLETION: 60\%
